Sys.setenv("HADOOP_STREAMING"="/Users/yuancalvin/hadoop-2.6.0/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar")
Sys.setenv(HADOOP_HOME="/Users/yuancalvin/hadoop-2.6.0")
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home")
library(rhbase)
library(rhdfs)
library(rmr2)
# Map each word with a keypair like (the , 1), and (mine, 1)
map_word <- function(k, lines){
wordsList <- strsplit(lines, '\\s')
words <- unlist(wordsList)
return(keyval(words, 1))
}
# For each word, we sum the total counts
reduce <- function(word, counts){
keyval(word, sum(counts))
}
wordcount <- function(input, output=NULL){
mapreduce(input=input, output = output, input.format = "text", map=map_word, reduce = reduce)
}
# Set up data source from hdfs
hdfs.root <- '/user/hang'
hdfs.data <- file.path(hdfs.root, 'data')
hdfs.out <- file.path(hdfs.root, 'out')
system.time(out <- wordcount(hdfs.data, hdfs.out))
# Always need to set the enviroment before running rHadoop
Sys.setenv("HADOOP_CMD"="/Users/yuancalvin/hadoop-2.6.0/bin/hadoop")
Sys.setenv("HADOOP_STREAMING"="/Users/yuancalvin/hadoop-2.6.0/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar")
Sys.setenv(HADOOP_HOME="/Users/yuancalvin/hadoop-2.6.0")
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home")
library(rhbase)
library(rhdfs)
library(rmr2)
# Map each word with a keypair like (the , 1), and (mine, 1)
map_word <- function(k, lines){
wordsList <- strsplit(lines, '\\s')
words <- unlist(wordsList)
return(keyval(words, 1))
}
# For each word, we sum the total counts
reduce <- function(word, counts){
keyval(word, sum(counts))
}
wordcount <- function(input, output=NULL){
mapreduce(input=input, output = output, input.format = "text", map=map_word, reduce = reduce)
}
# Set up data source from hdfs
hdfs.root <- '/user/hang'
hdfs.data <- file.path(hdfs.root, 'data')
hdfs.out <- file.path(hdfs.root, 'out')
system.time(out <- wordcount(hdfs.data, hdfs.out))
system.time(out <- wordcount(hdfs.data, hdfs.out))
result <- from.dfs(out)
results.df <- as.data.frame(result, stringsAsFactors = F)
colnames(results.df) <- c('word', 'count')
View(results.df)
View(results.df)
# We will compare the speed of r with Rhadoop on different sizes of data sets
# Kmeans using R on data set Iris
Iris = iris
Iris.features <- Iris
table(Iris$Species)
# setosa versicolor  virginica
# 50         50         50
# We know there are 3 different species in the data set
# So we make the clusters as 3.
Iris.features$Species <- NULL
result <- system.time(kmeans(Iris.features, 3))
# Kmeans
# Always need to set the enviroment before running rHadoop
Sys.setenv("HADOOP_CMD"="/Users/yuancalvin/hadoop-2.6.0/bin/hadoop")
Sys.setenv("HADOOP_STREAMING"="/Users/yuancalvin/hadoop-2.6.0/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar")
Sys.setenv(HADOOP_HOME="/Users/yuancalvin/hadoop-2.6.0")
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home")
# library(rhbase)
# library(rhdfs)
library(rmr2)
# This is an implemntation for Kmeans from rmr2 package
# @knitr kmeans-signature
kmeans.mr =
function(
P,
num.clusters,
num.iter,
combine,
in.memory.combine) {
## @knitr kmeans-dist.fun
dist.fun =
function(C, P) {
apply(
C,
1,
function(x)
colSums((t(P) - x)^2))}
## @knitr kmeans.map
kmeans.map =
function(., P) {
nearest = {
if(is.null(C))
sample(
1:num.clusters,
nrow(P),
replace = TRUE)
else {
D = dist.fun(C, P)
nearest = max.col(-D)}}
if(!(combine || in.memory.combine))
keyval(nearest, P)
else
keyval(nearest, cbind(1, P))}
## @knitr kmeans.reduce
kmeans.reduce = {
if (!(combine || in.memory.combine) )
function(., P)
t(as.matrix(apply(P, 2, mean)))
else
function(k, P)
keyval(
k,
t(as.matrix(apply(P, 2, sum))))}
## @knitr kmeans-main-1
C = NULL
for(i in 1:num.iter ) {
C =
values(
from.dfs(
mapreduce(
P,
map = kmeans.map,
reduce = kmeans.reduce)))
if(combine || in.memory.combine)
C = C[, -1]/C[, 1]
## @knitr end
#      points(C, col = i + 1, pch = 19)
## @knitr kmeans-main-2
if(nrow(C) < num.clusters) {
C =
rbind(
C,
matrix(
rnorm(
(num.clusters -
nrow(C)) * nrow(C)),
ncol = nrow(C)) %*% C) }}
C}
## @knitr end
## sample runs
##
out = list()
for(be in c("local", "hadoop")) {
rmr.options(backend = be)
out[[be]] =
## @knitr kmeans-run
kmeans.mr(
to.dfs(Iris.features),
num.clusters = 3,
num.iter = 1,
combine = FALSE,
in.memory.combine = FALSE)
## @knitr end
}
out
out = list()
for(be in c("local", "hadoop")) {
rmr.options(backend = be)
out[[be]] =
## @knitr kmeans-run
kmeans.mr(
to.dfs(Iris.features),
num.clusters = 3,
num.iter = 3,
combine = FALSE,
in.memory.combine = FALSE)
## @knitr end
}
out
result <- system.time(kmeans.mr(
to.dfs(Iris.features),
num.clusters = 3,
num.iter = 4,
combine = FALSE,
in.memory.combine = FALSE))
result
data <- list()
N<- 200000
for (n in 1:N){
data[[n]] = Iris.features
}
library(plyr)
data <- list()
# Number of duplicates
N<- 10000
for (n in 1:N){
data[[n]] = Iris.features
}
myNew <- ldply(data, rbind)
result <- system.time(kmeans(myNew, 3))
result
result <- system.time(kmeans.mr(
to.dfs(Iris.features),
num.clusters = 3,
num.iter = 4,
combine = FALSE,
in.memory.combine = FALSE))
result <- system.time(kmeans.mr(
to.dfs(myNew),
num.clusters = 3,
num.iter = 4,
combine = FALSE,
in.memory.combine = FALSE))
result
# Now let's have a really large data set
data <- list()
# Number of duplicates
N<- 200000
for (n in 1:N){
data[[n]] = Iris.features
}
myNew <- ldply(data, rbind)
# Now the data set is approximately 1.1GB
# > result
# user  system elapsed
# 10.178  15.067  31.432
result <- system.time(kmeans(myNew, 3))
result
result <- system.time(kmeans.mr(
to.dfs(Iris.features),
num.clusters = 3,
num.iter = 4,
combine = FALSE,
in.memory.combine = FALSE))
result
result <- system.time(kmeans.mr(
to.dfs(myNew),
num.clusters = 3,
num.iter = 4,
combine = FALSE,
in.memory.combine = FALSE))
result
Iris = iris
Iris.features <- Iris
table(Iris$Species)
View(Iris)
---
title: "test"
summary(cars)
This is a report on SparkR using R Markdown.
install.packages("rjava")
install.packages("rJava")
install.packages("rJava")
install.packages("rJava")
library(rJava)
install.packages(SparkR)
install.packages("SparkR")
Sys.setenv(SPARK_HOME= "~/spark-1.5.2")
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)
sc <- sparkR.init(master="local")
library(SparkR)
sc <- sparkR.init(master="local")
Sys.setenv(SPARK_HOME= "/Users/yuancalvin/spark-1.5.2")
Sys.setenv(SPARK_HOME= "/Users/yuancalvin/spark-1.5.2")
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)
sc <- sparkR.init(master="local")
lines <- textFile(sc, "hdfs://my_text_file")
lines <- SparkR:::textFile(sc, "hdfs://my_text_file")
numAs <- count(SparkR:::filterRDD(logData, function(s) { grepl("a", s) }))
numBs <- count(SparkR:::filterRDD(logData, function(s) { grepl("b", s) }))
paste("Lines with a: ", numAs, ", Lines with b: ", numBs, sep="")
sparkR.stop()
Sys.setenv(SPARK_HOME= "/Users/yuancalvin/spark-1.5.2")
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)
library("MASS")
data(cats)
cats
View(cats)
str(cats)
head(cats)
summary(cats)
help(plot)
help(abline)
Sys.setenv(SPARK_HOME= "/Users/yuancalvin/spark-1.5.2") # Set this to where sparkR is installed
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)
sc <- sparkR.init(master="local")
model_spark <- glm(Hwt ~ Bwt)
model_spark <- glm(cat$Hwt ~ cat$Bwt)
model_spark <- glm(cats$Hwt ~ cats$Bwt)
model_spark <- SparkR::glm(cats$Hwt ~ cats$Bwt)
model_spark <- SparkR::glm(cats$Hwt ~ cats$Bwt)
summary(model_spark)
model_spark
model_spark <- SparkR::glm(cats$Bwt ~ cats$Hwt)
model_spark
glm(cats$Bwt ~ cats$Hwt)
model_spark <- SparkR::glm(cats$Bwt ~ cats$Hwt)
plot(Hwt, Bwt, xlab = "Heart weight", ylab = "Body weight", main = "Body weight vs Heart weight")
attach(cats)
plot(Hwt, Bwt, xlab = "Heart weight", ylab = "Body weight", main = "Body weight vs Heart weight")
abline(model_spark)
str(cats)
class(cats)
logit <- glm (Sex ~ Bwt + Hwt)
logit <- glm (Sex ~ Bwt + Hwt, family = "binomial")
logit
summarize(logit)
summary(logit)
model_log <- SparkR::glm(Sex ~ Bwt + Hwt, family = "binomial")
model_log
catsDF <- collect(cats)
class(cats)
catsDF <- SparkR""collect(cats)
catsDF <- SparkR::collect(cats)
catsDF <- SparkR::collect(cats)
help(SparkR::collect)
help("collect")
newCats <- createDataFrame(sqlContext, cats)
newCats <- SparkR::createDataFrame(sqlContext, cats)
newCats <- SparkR::createDataFrame( cats)
help("createDataFrame")
sqlContext <- sparkRSQL.init(sc)
newCats <- SparkR::createDataFrame(sqlContext, cats)
class(newCats)
df <- SparkR::createDataFrame(sqlContext, cats)
head(df)
newCats <- SparkR::createDataFrame(sqlContext, cats)
head(newCats)
model_spark <- SparkR::glm(Bwt ~ Hwt, data = newCats)
summary(model_log)
summary(model)
summary(model_spark)
help("predict")
help("SparkR::predict")
??SparkR::predict
plot(Hwt, Bwt, xlab = "Heart weight", ylab = "Body weight", main = "Body weight vs Heart weight")
lin_spark <- SparkR::glm(Bwt ~ Hwt, data = newCats)
predications <- SparkR::predict(lin_spark, newCats)
SparkR::select(predications, "Bwt", "prediction")
head(SparkR::select(predications, "Bwt", "prediction"))
head(predications)
SparkR::head(predications)
head(SparkR::select(predications, "Bwt", "prediction"))
model_log <- SparkR::glm(Sex ~ Bwt + Hwt, data= newCats, family = "binomial")
model_log <- SparkR::glm(Sex ~ Bwt + Hwt, data= newCats, family = "binomial")
predications <- SparkR::predict(lin_spark, newCats)
model_log <- SparkR::glm(Sex ~ Bwt + Hwt, data= newCats, family = "binomial")
help(glm)
lin_spark <- SparkR::glm(Bwt ~ Hwt, data = newCats, family = gaussian())
lin_spark <- SparkR::glm(Bwt ~ Hwt, data = newCats, family = gaussian)
lin_spark <- SparkR::glm(Bwt ~ Hwt, data = newCats, family = "gaussian"")
;
''
""
lin_spark <- SparkR::glm(Bwt ~ Hwt, data = newCats, family = "gaussian")
predications <- SparkR::predict(lin_spark, newCats)
head(SparkR::select(predications, "Bwt", "prediction"))
model_log <- SparkR::glm(Sex ~ Bwt + Hwt, data= newCats, family = "binomial")
model_log <- SparkR::glm(Sex ~ Bwt + Hwt, data= newCats, family = "binomial")
head(newCats)
model_log <- SparkR::glm(Bwt ~ Hwt, data= newCats, family = "binomial")
model_log <- SparkR::glm(Sex ~ Bwt + Hwt, data= newCats, family = "binomial")
head(cats)
as.numeric(cats$Sex)
cats$Sex <- as.numeric(cats$Sex)
newCats <- SparkR::createDataFrame(sqlContext, cats)
model_log <- SparkR::glm(Sex ~ Bwt + Hwt, data= newCats, family = "binomial")
head(newCats)
View(cats)
levels(cats$Sex)
toConvert <- function(x){
if (x == 'F'){
x <- 0
} else {
x <-1
}
}
a <- c (1, 2, 3,4 )
a
lapply(a, toConvert)
lapply(cats$Sex, toConvert)
cats$Sex <- lapply(cats$Sex, toConvert)
newCats <- SparkR::createDataFrame(sqlContext, cats)
cats$Sex <- factor(cats$Sex)
cats$Sex
View(cats)
data(cats)
head(cats)
View(cats)
View(cats)
ll <- lapply(cats$Sex, toConvert)
ll
unlist(ll)
cats$Sex = ll
newCats <- SparkR::createDataFrame(sqlContext, cats)
View(cats)
class(cats$Sex)
as.numeric(cats$Sex)
cats$Sex <- as.numeric(cats$Sex)
cats$Sex <- factor(cats$Sex)
newCats <- SparkR::createDataFrame(sqlContext, cats)
head(newCats)
model_log <- SparkR::glm(Sex ~ Bwt + Hwt, data= newCats, family = "binomial")
cats$Sex <- as.numeric(cats$Sex)
newCats <- SparkR::createDataFrame(sqlContext, cats)
model_log <- SparkR::glm(Sex ~ Bwt + Hwt, data= newCats, family = "binomial")
levels(cats$Sex)
head(newCats)
plot(Hwt, Bwt, xlab = "Heart weight", ylab = "Body weight", main = "Body weight vs Heart weight")
plot(Hwt, Bwt, xlab = "Heart weight", ylab = "Body weight", data=newCats main = "Body weight vs Heart weight")
library(ggplot2)
install.packages("ggplot2")
library(ggplot2)
help("ggplot")
ggplot(cats)
ggplot(cats, ase(x=factor(Sex)))
ggplot(cats, aes(x=factor(Sex)))
View(cats)
View(cats)
data(cats)
head(cats)
ggplot(cats, aes(x=factor(Sex)))
View(cats)
ggplot(cats)
View(cats)
cats$Sex <- unlist(lapply(cats$Sex, toConvert))
View(cats)
newCats <- SparkR::createDataFrame(sqlContext, cats)
model_log <- SparkR::glm(Sex ~ Bwt + Hwt, data= newCats, family = "binomial")
predications <- SparkR::predict(model_log, newCats)
head(SparkR::select(predications, "Bwt", "prediction"))
head(SparkR::select(predications, "Sex", "prediction"))
smp_size <- floor (0.70 * nrow(cats))
set.seed(100)
train_id <- sample(seq_len(nrow(cats)), size = smp_size)
train <- cats[train_id, ]
test <- cats[-train_id, ]
train_id <- sample(seq_len(nrow(cats)), size = smp_size)
train_id <- sample(seq_len(nrow(cats)), size = smp_size)
help("sample")
train_id <- sample(seq_len(nrow(cats)), smp_size, replace = FALSE)
train_id <- sample(seq_len(nrow(cats)), smp_size)
train_id <- sample.init(seq_len(nrow(cats)), smp_size)
train_id <- sample.int(seq_len(nrow(cats)), smp_size)
seq_len(nrow(cats))
require(caTools)
sample = sample.split(cats, SplitRatio = .70)
train = subset(data, sample == TRUE)
test = subset(data, sample == FALSE)
train = subset(cats, sample == TRUE)
test = subset(cats, sample == FALSE)
sample = sample.split(cats$Sex, SplitRatio = .70)
sample
sample = sample.split(cats$Sex, SplitRatio = .70)
train = subset(cats, sample == TRUE)
test = subset(cats, sample == FALSE)
sample = sample.split(cats$Sex, SplitRatio = .70)
train = subset(cats, sample == TRUE)
test = subset(cats, sample == FALSE)
require(caTools)
train = subset(cats, sample == TRUE)
test = subset(cats, sample == FALSE)
train_id <- sample.int(seq_len(nrow(cats)), smp_size)
seq_len(3)
seq_len(nrow(cats))
sample(seq_len(nrow(cats)), smp_size)
smp_size <- floor (0.70 * nrow(cats))
sample(seq_len(nrow(cats)), smp_size)
sample(c(0,1), 100, replace = TRUE)
sample(c(0,1), 100, replace = TRUE)
sample(c(0,1), 100, replace = TRUE)
set.seed(100)
sample(c(0,1), 100, replace = TRUE)
library(caret)
install.packages("caret")
trainIndex <- createDataPartition(cats$Sex, p = .7,
list = FALSE,
times = 1)
library(caret)
trainIndex <- createDataPartition(cats$Sex, p = .7,
list = FALSE,
times = 1)
head(trainIndex)
train <- cats[trainIndex, ]
test <- cats[-trainIndex, ]
trainRDD <- SparkR::createDataFrame(sqlContext, train)
testRDD <- SparkR::createDataFrame(sqlContext, test)
lin_spark <- SparkR::glm(Bwt ~ Hwt, data = trainRDD, family = "gaussian")
predications <- SparkR::predict(lin_spark, testRDD)
head(SparkR::select(testRDD, "Bwt", "prediction"))
lin_spark <- SparkR::glm(Bwt ~ Hwt, data = trainRDD, family = "gaussian")
predications <- SparkR::predict(lin_spark, testRDD)
head(SparkR::select(testRDD, "Bwt", "prediction"))
head(SparkR::select(predications, "Bwt", "prediction"))
model_log <- SparkR::glm(Sex ~ Bwt + Hwt, data= trainRDD, family = "binomial")
predications <- SparkR::predict(model_log, testRDD)
head(SparkR::select(predications, "Sex", "prediction"))
showDF(SparkR::select(predications, "Sex", "prediction"))
setwd("~/Kaggle/number/dataset/")
library(randomForest)
set.seed(0)
numTrain <- 42000
numTrees <- 25
library(data.table)
train <- fread('train.csv', header = T, sep = ',')
test <- fread('test.csv', header = T, sep = ',')
rows <- sample(1:nrow(train), 1000)
labels <- train[rows, label]
train1 <- subset(train, select = -label)[rows, ]
test1 <- subset(train, select = label)[-rows, ]
numTrain <- 22000
train <- fread('train.csv', header = T, sep = ',')
